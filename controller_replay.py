#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Jetson과 아두이노 RC카 연결 코드 (DQN Replay 버전)

L298N 모터 드라이버와 엔코더를 사용하는 RC카를 Jetson에서 제어합니다.
dqn_Replay.py의 새로운 DQN 구조에 맞게 수정됨.
"""

import serial
import time
import threading
import queue
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import json
import os
from collections import deque
import random
import psutil

# OpenCV GUI 비활성화
os.environ['OPENCV_VIDEOIO_PRIORITY_V4L2'] = '1'
cv2.setNumThreads(1)

# Jetson Orin에서 CUDA 메모리 최적화
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
    os.environ["CUDA_CACHE_DISABLE"] = "0"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# 새로운 DQN 구조 정의
class LaneDetector:
    def __init__(self, slope_threshold=0.2):
        self.prev_lane = 1  # 초기값: 오른쪽 차선
        self.slope_threshold = slope_threshold
        # 빨강 HSV 범위
        self.lower_red1 = np.array([0, 70, 50])
        self.upper_red1 = np.array([10, 255, 255])
        self.lower_red2 = np.array([170, 70, 50])
        self.upper_red2 = np.array([180, 255, 255])

    def process_frame(self, frame):
        height, width = frame.shape[:2]

        # ----------------------
        # HSV 변환 후 노랑+흰색 추출
        # ----------------------
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        mask_white = cv2.inRange(hsv, np.array([0, 0, 230]), np.array([180, 25, 255]))
        mask_yellow = cv2.inRange(
            hsv, np.array([15, 80, 100]), np.array([35, 255, 255])
        )
        lane_mask = cv2.bitwise_or(mask_white, mask_yellow)

        # ----------------------
        # 노란색 물체 기반 좌/우 판단
        # ----------------------
        yellow_mask = cv2.inRange(
            hsv, np.array([15, 80, 100]), np.array([35, 255, 255])
        )
        left_half = yellow_mask[:, : width // 2]
        right_half = yellow_mask[:, width // 2 :]
        left_count = cv2.countNonZero(left_half)
        right_count = cv2.countNonZero(right_half)

        # lane_state 결정: 오른쪽, 왼쪽
        if left_count > right_count and left_count > 0:
            lane_state = "right"
        elif right_count > left_count and right_count > 0:
            lane_state = "left"
        else:
            lane_state = self.prev_lane  # 이전 차선 유지
        self.prev_lane = lane_state

        # ----------------------
        # 빨강 물체 검출
        # ----------------------
        mask_red1 = cv2.inRange(hsv, self.lower_red1, self.upper_red1)
        mask_red2 = cv2.inRange(hsv, self.lower_red2, self.upper_red2)
        mask_red = cv2.bitwise_or(mask_red1, mask_red2)
        red_pixels = cv2.countNonZero(mask_red)
        red_ratio = red_pixels / (height * width)

        return lane_state, red_ratio

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),  # 과적합 방지
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, action_dim),
        )

    def forward(self, x):
        return self.fc(x)

class DQNAgent:
    def __init__(
        self, state_dim, action_dim=2, device="cpu", cql_alpha=1e-4, lr=1e-4, gamma=0.99
    ):
        if torch.cuda.is_available():
            device = "cuda"
        self.device = device
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.cql_alpha = cql_alpha

        # 네트워크 초기화
        self.policy_net = DQN(state_dim, action_dim).to(device)
        self.target_net = DQN(state_dim, action_dim).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())

        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)

        # 온라인 학습용 Replay Buffer
        self.replay_buffer = deque(maxlen=10000)
        self.batch_size = 32

        # 액션 출력 관련 상태
        self.prev_action = 1
        self.same_count = 0
        self.last_output = None

    def train_online_step(self, state, reward, next_state, done):
        state_tensor = (
            torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)
        )
        next_state_tensor = (
            torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device)
        )

        # Q값 계산
        self.policy_net.eval()
        q_values = self.policy_net(state_tensor)
        action = torch.argmax(q_values, dim=1).item()

        # --- 액션 출력 결정 ---
        if self.prev_action == 0 and action == 1:
            output = "right"
        elif self.prev_action == 1 and action == 0:
            output = "left"
        else:
            output = "straight"

        print(f"Action: {action}, Output: {output}, Q-values: {q_values.cpu().detach().numpy()}")
        self.prev_action = action

        # --- Replay Buffer에 저장 ---
        self.replay_buffer.append((state, action, reward, next_state, done))

        # --- 온라인 학습 ---
        if len(self.replay_buffer) >= self.batch_size:
            batch = random.sample(self.replay_buffer, self.batch_size)
            states = torch.tensor([b[0] for b in batch], dtype=torch.float32).to(
                self.device
            )
            actions = torch.tensor([b[1] for b in batch], dtype=torch.long).to(
                self.device
            )
            rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32).to(
                self.device
            )
            next_states = torch.tensor([b[3] for b in batch], dtype=torch.float32).to(
                self.device
            )
            dones = torch.tensor([b[4] for b in batch], dtype=torch.bool).to(
                self.device
            )

            current_q = (
                self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()
            )
            with torch.no_grad():
                next_q = self.target_net(next_states).max(1)[0]
                target_q = rewards + self.gamma * (1 - dones.float()) * next_q

            all_q = self.policy_net(states)
            logsumexp_q = torch.logsumexp(all_q, dim=1)
            cql_penalty = (logsumexp_q - current_q).mean()

            loss = nn.MSELoss()(current_q, target_q) + self.cql_alpha * cql_penalty

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            # 타겟 네트워크 업데이트 (확률적으로)
            if random.random() < 0.01:
                self.target_net.load_state_dict(self.policy_net.state_dict())

        return action, output

class ArduinoRCCarController:
    def __init__(self, port='/dev/ttyUSB0', baudrate=9600):
        """아두이노 RC카 컨트롤러 초기화"""
        self.port = port
        self.baudrate = baudrate
        self.serial_conn = None
        self.connected = False
        
        # RC카 파라미터 (아두이노 코드와 동일)
        self.WHEEL_DIAMETER_CM = 6.5
        self.COUNTS_PER_REV = 20
        self.DRIVE_PWM = 180
        self.STOP_BRAKE_TIME_MS = 120
        
        # 현재 상태
        self.current_speed = 0
        self.current_steering = 0
        self.left_count = 0
        self.right_count = 0
        
        # 명령 큐
        self.command_queue = queue.Queue()
        self.response_queue = queue.Queue()
        
    def connect(self):
        """아두이노와 시리얼 연결"""
        try:
            self.serial_conn = serial.Serial(
                port=self.port,
                baudrate=self.baudrate,
                timeout=1
            )
            time.sleep(2)  # 아두이노 초기화 대기
            self.connected = True
            print(f"아두이노 연결 성공: {self.port}")
            
            # 통신 스레드 시작
            self.start_communication_thread()
            return True
            
        except Exception as e:
            print(f"아두이노 연결 실패: {e}")
            return False
            
    def start_communication_thread(self):
        """통신 스레드 시작"""
        self.comm_thread = threading.Thread(target=self.communication_loop)
        self.comm_thread.daemon = True
        self.comm_thread.start()
        
    def communication_loop(self):
        """시리얼 통신 루프"""
        while self.connected:
            try:
                # 명령 전송
                if not self.command_queue.empty():
                    command = self.command_queue.get_nowait()
                    self.serial_conn.write(command.encode('utf-8'))
                    self.serial_conn.write(b'\n')
                    
                # 응답 수신
                if self.serial_conn.in_waiting > 0:
                    response = self.serial_conn.readline().decode('utf-8').strip()
                    if response:
                        self.response_queue.put(response)
                        
            except Exception as e:
                print(f"통신 오류: {e}")
                break
                
            time.sleep(0.01)
            
    def send_simple_command(self, command):
        """단순 명령 전송 (아두이노는 단순 문자열 명령만 지원)"""
        if not self.connected:
            return False
            
        try:
            self.command_queue.put(command)
            return True
        except Exception as e:
            print(f"단순 명령 전송 실패: {e}")
            return False
            
    def move_forward(self, speed=None, distance_cm=None):
        """전진"""
        return self.send_simple_command('MOVE_FORWARD')
        
    def move_backward(self, speed=None, distance_cm=None):
        """후진"""
        return self.send_simple_command('STOP')
        
    def turn_left(self, angle_degrees=None):
        """좌회전"""
        return self.send_simple_command('TURN_LEFT')
        
    def turn_right(self, angle_degrees=None):
        """우회전"""
        return self.send_simple_command('TURN_RIGHT')
        
    def stop(self):
        """정지"""
        return self.send_simple_command('STOP')
        
    def get_encoder_data(self):
        """엔코더 데이터 요청"""
        return self.send_simple_command('STATUS')
        
    def set_speed(self, left_speed, right_speed):
        """좌우 모터 속도 개별 설정 (아두이노는 고정 속도 사용)"""
        return True
        
    def get_status(self):
        """상태 정보 요청"""
        return self.send_simple_command('STATUS')

class JetsonAutonomousController:
    def __init__(self, arduino_port='/dev/ttyUSB0'):
        """Jetson 자율주행 컨트롤러 (DQN Replay 버전)"""
        # 아두이노 RC카 컨트롤러
        self.rc_car = ArduinoRCCarController(port=arduino_port)
        
        # 새로운 구조의 LaneDetector
        self.lane_detector = LaneDetector()
        
        # 새로운 DQN Agent 생성
        self.dqn_agent = DQNAgent(state_dim=4, action_dim=2, device=device)
        
        # See3CAM_CU27 카메라 설정 (GUI 없이 실행)
        self.cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
        
        # GUI 없이 실행을 위한 설정
        self.headless_mode = True
        
        # OpenCV 백엔드 설정 (GUI 완전 비활성화)
        cv2.setUseOptimized(True)
        cv2.setNumThreads(1)
        
        # 실제 설정값 확인
        actual_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        print(f"See3CAM_CU27 설정: {actual_width}x{actual_height}")

        # 실행 상태
        self.running = False
        self.autonomous_mode = False
        
        # 액션 매핑 (새로운 구조)
        self.action_map = {
            'left': self.action_left,      # 좌회전
            'straight': self.action_center,   # 직진
            'right': self.action_right     # 우회전
        }
        
        # 상태 추출 관련
        self.prev_clane = 1  # 이전 차선
        
    def action_left(self):
        """자회전 액션"""
        print("실행: 좌회전")
        self.rc_car.turn_left()
        
    def action_center(self):
        """직진 액션"""
        print("실행: 직진")
        self.rc_car.move_forward()
        
    def action_right(self):
        """우회전 액션"""
        print("실행: 우회전")
        self.rc_car.turn_right()
        
    def get_state_replay_format(self, frame):
        """dqn_Replay.py 형식에 맞게 상태 추출"""
        height, width = frame.shape[:2]
        
        # 차선과 빨강 비율 계산
        lane_state, red_ratio = self.lane_detector.process_frame(frame)
        
        # over_line 계산 (바탕 픽셀 판단)
        bottom_center_pixel = frame[height - 1, width // 2]
        over_line = float(np.all(bottom_center_pixel > 240))
        
        # 이전 차선 숫자로 변환: left=0, right=1
        prev_lane_num = 0 if self.prev_clane == "left" else 1
        
        # 현재 차선을 저장
        self.prev_clane = lane_state
        
        print(f"Debug - lane_state: {lane_state}, red_ratio: {red_ratio:.4f}, over_line: {over_line}, prev_lane_num: {prev_lane_num}")
        
        # state = [red_ratio, over_line, prev_lane_num, move]
        # move는 DQNAgent에서 결정되므로 임시로 1 설정
        state = np.array([red_ratio, over_line, prev_lane_num, 1], dtype=np.float32)
        return state
        
    def calculate_reward(self, state, prev_state=None):
        """보상 계산"""
        reward = 3.0  # 기본 보상
        red_ratio = state[0]
        
        # 빨강 물체가 있을 때 추가 보상
        if red_ratio > 0.06:
            reward += 20
            
        return reward
        
    def autonomous_control_loop(self):
        """자율주행 제어 루프 (DQN Replay 버전)"""
        prev_state = None
        
        while self.running and self.autonomous_mode:
            try:
                ret, frame = self.cap.read()
                if ret:
                    # 상태 추출 (새로운 형식)
                    state = self.get_state_replay_format(frame)
                    
                    # 이전 상태가 있으면 보상 계산
                    if prev_state is not None:
                        reward = self.calculate_reward(state, prev_state)
                    else:
                        reward = 3.0  # 초기 보상
                    
                    # 종료 조건 확인
                    done = False
                    if state[1] > 0:  # over_line
                        print("종료 조건: 라인 넘어감")
                        done = True
                    if state[0] > 0.2:  # 빨강 물체 기준
                        print("종료 조건: 빨강 물체 접근")
                        done = True
                    
                    # DQN Agent로 액션 결정
                    if not done:
                        action, output = self.dqn_agent.train_online_step(state, reward, state, done)
                        
                        # 액션 실행
                        self.action_map[output]()
                    else:
                        self.rc_car.stop()
                        print("자율주행 종료")
                        break
                    
                    # 이전 상태 업데이트
                    prev_state = state.copy()
                    
                # GUI 없이 실행 - 디버그 정보만 출력
                
            except Exception as e:
                print(f"자율주행 제어 오류: {e}")
                self.rc_car.stop()
                
            time.sleep(0.1)  # Jetson Orin에 최적화된 간격
            
    def start_autonomous(self):
        """자율주행 시작"""
        if not self.rc_car.connected:
            print("아두이노가 연결되지 않았습니다.")
            return False
            
        print("DQN Replay 자율주행 모드 시작...")
        self.autonomous_mode = True
        self.running = True
        
        # 자율주행 스레드 시작
        self.autonomous_thread = threading.Thread(target=self.autonomous_control_loop)
        self.autonomous_thread.daemon = True
        self.autonomous_thread.start()
        
        return True
        
    def stop_autonomous(self):
        """자율주행 중지"""
        print("자율주행 모드 중지...")
        self.autonomous_mode = False
        self.running = False
        self.rc_car.stop()
        
    def manual_control(self):
        """수동 제어 모드"""
        print("수동 제어 모드 (키보드 입력)")
        print("w: 전진, s: 후진, a: 좌회전, d: 우회전, x: 정지, q: 종료")
        
        while self.running:
            key = input("명령 입력: ").lower()
            
            if key == 'w':
                self.rc_car.move_forward()
                print("전진")
            elif key == 's':
                self.rc_car.move_backward()
                print("후진")
            elif key == 'a':
                self.rc_car.turn_left()
                print("좌회전")
            elif key == 'd':
                self.rc_car.turn_right()
                print("우회전")
            elif key == 'x':
                self.rc_car.stop()
                print("정지")
            elif key == 'q':
                break
            elif key == 'auto':
                self.start_autonomous()
                
    def start(self):
        """시스템 시작"""
        print("Jetson 아두이노 RC카 제어 시스템 (DQN Replay 버전)")
        
        # 아두이노 연결
        if not self.rc_car.connect():
            print("아두이노 연결 실패. 수동 모드로 진행합니다.")
            return False
            
        self.running = True
        
        # 모드 선택
        print("모드를 선택하세요:")
        print("1. 수동 제어")
        print("2. DQN 자율주행 (Replay 버전)")
        
        choice = input("선택 (1/2): ")
        
        if choice == '1':
            self.manual_control()
        elif choice == '2':
            self.start_autonomous()
            
            # 메인 스레드에서 키보드 입력 대기
            try:
                while self.running:
                    key = input("Press 'q' to quit, 's' to stop: ")
                    if key == 'q':
                        break
                    elif key == 's':
                        self.stop_autonomous()
                        
            except KeyboardInterrupt:
                print("중단됨")
                
        return True
        
    def cleanup(self):
        """정리"""
        self.running = False
        self.autonomous_mode = False
        self.rc_car.stop()
        if self.rc_car.serial_conn:
            self.rc_car.serial_conn.close()
        if self.cap:
            self.cap.release()
        # Jetson Orin 메모리 정리
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def optimize_jetson_performance():
    """Jetson Orin 성능 최적화"""
    print("Jetson Orin 성능 최적화 적용 중...")
    
    # CUDA 설정 최적화
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.cuda.empty_cache()
        
        os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
        os.environ["CUDA_CACHE_DISABLE"] = "0"
        
        print("CUDA 최적화 완료")
    
    # OpenCV 최적화
    cv2.setNumThreads(4)
    print("OpenCV 최적화 완료")

def monitor_jetson_performance():
    """Jetson Orin 성능 모니터링"""
    print("=== Jetson Orin 성능 모니터링 ===")
    
    # CPU 정보
    cpu_percent = psutil.cpu_percent(interval=1)
    print(f"CPU 사용률: {cpu_percent}%")
    
    # 메모리 정보
    memory = psutil.virtual_memory()
    print(f"시스템 메모리 사용률: {memory.percent}%")
    print(f"사용 가능한 메모리: {memory.available / 1024**3:.1f} GB")
    
    # GPU 정보
    if torch.cuda.is_available():
        print(f"GPU 메모리 사용량: {torch.cuda.memory_allocated() / 1024**3:.3f} GB")
        print(f"GPU 메모리 예약량: {torch.cuda.memory_reserved() / 1024**3:.3f} GB")

def main():
    """메인 함수"""
    print("Jetson 아두이노 RC카 제어 시스템 (DQN Replay 버전)")
    
    # 성능 최적화
    optimize_jetson_performance()
    monitor_jetson_performance()
    
    # 아두이노 포트 확인
    import glob
    ports = glob.glob('/dev/tty[A-Za-z]*')
    print("사용 가능한 포트:")
    for port in ports:
        print(f"  {port}")
        
    # 포트 선택
    arduino_port = input("아두이노 포트를 입력하세요 (예: /dev/ttyUSB0): ")
    if not arduino_port:
        arduino_port = '/dev/ttyACM0'
        
    # 컨트롤러 초기화
    controller = JetsonAutonomousController(arduino_port)
    
    try:
        # 시스템 시작
        controller.start()
    except Exception as e:
        print(f"오류 발생: {e}")
    finally:
        controller.cleanup()

if __name__ == "__main__":
    main()