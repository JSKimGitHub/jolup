{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSKimGitHub/testfile/blob/main/0916.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "sxjgFNzCEcez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4ea197-ef97-46af-affc-97720dc95b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ObstacleDetector:\n",
        "    def find_red_area(self, frame):\n",
        "        \"\"\"\n",
        "        frame: BGR 이미지 (OpenCV로 읽은 이미지)\n",
        "        return: 빨간색 물체의 넓이 (픽셀 수), 없으면 0\n",
        "        \"\"\"\n",
        "        # 1. BGR -> HSV 변환\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # 2. 빨간색 HSV 범위 정의\n",
        "        lower_red1 = np.array([0, 120, 70])\n",
        "        upper_red1 = np.array([10, 255, 255])\n",
        "        lower_red2 = np.array([170, 120, 70])\n",
        "        upper_red2 = np.array([180, 255, 255])\n",
        "\n",
        "        # 3. 마스크 생성\n",
        "        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
        "        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
        "        mask = mask1 + mask2\n",
        "\n",
        "        # 4. 노이즈 제거\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((5,5), np.uint8))\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, np.ones((5,5), np.uint8))\n",
        "\n",
        "        # 5. 컨투어 찾기\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if len(contours) == 0:\n",
        "            return 0  # 빨간색 물체 없음 → 넓이 0 반환\n",
        "\n",
        "        # 6. 가장 큰 컨투어의 면적 계산\n",
        "        c = max(contours, key=cv2.contourArea)\n",
        "        area = cv2.contourArea(c)\n",
        "\n",
        "        return area"
      ],
      "metadata": {
        "id": "mvC4neBKju2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LaneDetector:\n",
        "    def __init__(self):\n",
        "        self.prev_lanes = [None, None]  # [왼쪽 차선, 오른쪽 차선]\n",
        "        self.img_center = None\n",
        "        self.margin = 50  # 상태 판정 margin\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        height, width = frame.shape[:2]\n",
        "        self.img_center = width // 2\n",
        "\n",
        "        # ----------------------\n",
        "        # 1️⃣ Gray + Blur\n",
        "        # ----------------------\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # 흰색 범위 (밝은 영역)\n",
        "        lower_white = np.array([0, 0, 200])\n",
        "        upper_white = np.array([180, 30, 255])\n",
        "        mask_white = cv2.inRange(hsv, lower_white, upper_white)\n",
        "\n",
        "        # 노란색 범위\n",
        "        lower_yellow = np.array([15, 80, 100])\n",
        "        upper_yellow = np.array([35, 255, 255])\n",
        "        mask_yellow = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
        "\n",
        "        # 두 마스크 합치기\n",
        "        mask = cv2.bitwise_or(mask_white, mask_yellow)\n",
        "\n",
        "        # 원본에서 색상만 추출\n",
        "        result = cv2.bitwise_and(frame, frame, mask=mask)\n",
        "\n",
        "        gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
        "        blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
        "\n",
        "        # ----------------------\n",
        "        # 2️⃣ Canny 엣지\n",
        "        # ----------------------\n",
        "        edges = cv2.Canny(blur, 50, 150)\n",
        "\n",
        "        # ----------------------\n",
        "        # 3️⃣ ROI 적용\n",
        "        # ----------------------\n",
        "        mask = np.zeros_like(edges)\n",
        "        roi = np.array([[\n",
        "            (0, height),\n",
        "            (0, int(height*0.6)),\n",
        "            (width, int(height*0.6)),\n",
        "            (width, height)\n",
        "        ]], np.int32)\n",
        "        cv2.fillPoly(mask, roi, 255)\n",
        "        edges_roi = cv2.bitwise_and(edges, mask)\n",
        "\n",
        "        # ----------------------\n",
        "        # 4️⃣ 허프 직선\n",
        "        # ----------------------\n",
        "        lines = cv2.HoughLinesP(edges_roi, 1, np.pi/180, 50, minLineLength=20, maxLineGap=100)\n",
        "\n",
        "        # ----------------------\n",
        "        # 5️⃣ slope 기준 left/right 분류\n",
        "        # ----------------------\n",
        "        left_lines, right_lines = [], []\n",
        "        if lines is not None:\n",
        "            for x1, y1, x2, y2 in lines[:, 0]:\n",
        "                slope = (y2 - y1) / (x2 - x1 + 1e-6)\n",
        "                if slope < -0.5:\n",
        "                    left_lines.append((x1, y1, x2, y2))\n",
        "                elif slope > 0.5:\n",
        "                    right_lines.append((x1, y1, x2, y2))\n",
        "\n",
        "        # ----------------------\n",
        "        # 6️⃣ 화면 중심에 가장 가까운 안쪽 선 선택\n",
        "        # ----------------------\n",
        "        left_inner = max(left_lines, key=lambda l: (l[0]+l[2])/2) if left_lines else None\n",
        "        right_inner = min(right_lines, key=lambda l: (l[0]+l[2])/2) if right_lines else None\n",
        "\n",
        "        lanes = [None, None]  # [left, right]\n",
        "\n",
        "        for line_inner, idx in [(left_inner, 0), (right_inner, 1)]:\n",
        "            if line_inner is not None:\n",
        "                x1, y1, x2, y2 = line_inner\n",
        "                lane = [(x1, y1), (x2, y2)]  # 검출된 직선 그대로 사용\n",
        "\n",
        "                # 이전 프레임과 스무딩\n",
        "                if self.prev_lanes[idx] is not None:\n",
        "                    lane = [(\n",
        "                        (lane[i][0] + self.prev_lanes[idx][i][0]) // 2,\n",
        "                        (lane[i][1] + self.prev_lanes[idx][i][1]) // 2\n",
        "                    ) for i in range(len(lane))]\n",
        "\n",
        "                self.prev_lanes[idx] = lane\n",
        "                lanes[idx] = lane  # 상태 계산용\n",
        "\n",
        "        # ----------------------\n",
        "        # 6️⃣ 상태 계산\n",
        "        # ----------------------\n",
        "        lane_state = 1  # 기본 center\n",
        "\n",
        "        if lanes[0] is not None and lanes[1] is not None:\n",
        "            left_center = (lanes[0][0][0] + lanes[0][1][0]) // 2\n",
        "            right_center = (lanes[1][0][0] + lanes[1][1][0]) // 2\n",
        "            lane_center = (left_center + right_center) // 2\n",
        "\n",
        "            if abs(lane_center - self.img_center) < self.margin:\n",
        "                lane_state = 1  # center\n",
        "            elif lane_center < self.img_center:\n",
        "                lane_state = 0  # left\n",
        "            else:\n",
        "                lane_state = 2  # right\n",
        "\n",
        "        return lanes, lane_state\n"
      ],
      "metadata": {
        "id": "TLp72FqnYJeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN data collector"
      ],
      "metadata": {
        "id": "qleg2qWlkKKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OfflineDataCollector:\n",
        "    def __init__(self, lane_detector, obstacle_detector):\n",
        "        self.lane_detector = lane_detector\n",
        "        self.obstacle_detector = obstacle_detector\n",
        "\n",
        "    def _get_state(self, frame, car_x):\n",
        "        \"\"\"주어진 프레임에서 상태(state) 계산\"\"\"\n",
        "        # 장애물 정보\n",
        "        area= self.obstacle_detector.find_red_area(frame)\n",
        "\n",
        "        lanes, act= self.lane_detector.process_frame(frame)\n",
        "        lanes = np.array(lanes, dtype=object)\n",
        "        left_lane, right_lane = lanes\n",
        "\n",
        "        left_x = min(left_lane[0][0], left_lane[1][0]) if left_lane is not None else 0\n",
        "        right_x = max(right_lane[0][0], right_lane[1][0]) if right_lane is not None else frame.shape[1]\n",
        "\n",
        "        state = np.array([\n",
        "            left_x / frame.shape[1],\n",
        "            right_x / frame.shape[1],\n",
        "            (left_x + right_x) / (2 * frame.shape[1]),  # 차선 중앙\n",
        "            car_x / frame.shape[1],\n",
        "            area / (frame.shape[0] * frame.shape[1])  # 프레임에 대한 빨간색의 비율\n",
        "       ], dtype=np.float32)\n",
        "\n",
        "        return state, act\n",
        "\n",
        "    def _calculate_reward(self, state):\n",
        "        \"\"\"주어진 상태에서 보상 계산\"\"\"\n",
        "        reward = 0.0\n",
        "\n",
        "        lane_center = state[2]\n",
        "        car_position = state[3]\n",
        "        distance_from_center = abs(car_position - lane_center)\n",
        "\n",
        "        # 차선 중앙 유지 보상\n",
        "        if distance_from_center < 0.1:\n",
        "            reward += 10.0\n",
        "        elif distance_from_center < 0.2:\n",
        "            reward += 5.0\n",
        "        else:\n",
        "            reward -= 5.0\n",
        "\n",
        "        # 차선 이탈 페널티\n",
        "        if distance_from_center > 0.4:\n",
        "            reward -= 20.0\n",
        "\n",
        "        # 안정적 주행 기본 보상\n",
        "        reward += 5.0\n",
        "\n",
        "        # 장애물과의 거리 보상\n",
        "        norm_area = state[4]\n",
        "        if norm_area > 0.7:\n",
        "            reward -= 70.0\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def collect_from_frames(self, frames, car_x_init=None, actions_taken=None):\n",
        "        \"\"\"\n",
        "        frames에서 state/action/reward/next_state/done 리스트 생성\n",
        "        frames : 비디오 프레임 리스트\n",
        "        car_x_init : 초기 차량 위치 (없으면 화면 중앙)\n",
        "        actions_taken : 이미 결정된 action 리스트 (없으면 간단 규칙 적용)\n",
        "        \"\"\"\n",
        "        state_list = []\n",
        "        action_list = []\n",
        "        reward_list = []\n",
        "        next_state_list = []\n",
        "        done_list = []\n",
        "\n",
        "        car_x = frames[0].shape[1] // 2\n",
        "\n",
        "        valid_indices = [i for i in range(len(frames)-1) if i % 4 == 0]\n",
        "        for idx in valid_indices:\n",
        "            frame = frames[idx]\n",
        "            if(idx+4>=len(frames)):\n",
        "                break\n",
        "            next_frame = frames[idx + 4]\n",
        "\n",
        "            # 현재 상태\n",
        "            state, act= self._get_state(frame, car_x)\n",
        "            #cv2.putText(draw, f\"act: {act}\", (10, 30),\n",
        "             #     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "            # 다음 상태\n",
        "            next_state, next_act= self._get_state(next_frame, car_x)\n",
        "\n",
        "            # 보상 계산\n",
        "            reward = self._calculate_reward(state)\n",
        "\n",
        "            # done 여부\n",
        "            done = False\n",
        "\n",
        "            if abs(next_state[3] - next_state[2]) > 0.5:  # 차선 벗어나면 종료\n",
        "                done = True\n",
        "\n",
        "            if idx +4 >= len(frames):  # 마지막 프레임\n",
        "                done = True\n",
        "\n",
        "            # 리스트 저장\n",
        "            state_list.append(state)\n",
        "            action_list.append(act)\n",
        "            reward_list.append(reward)\n",
        "            next_state_list.append(next_state)\n",
        "            done_list.append(done)\n",
        "\n",
        "\n",
        "        return state_list, action_list, reward_list, next_state_list, done_list\n"
      ],
      "metadata": {
        "id": "rHYdz-nSWUcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),  # 과적합 방지\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "gL9GJIXWRQ-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_offline_dqn(state_list, action_list, reward_list, next_state_list, done_list,\n",
        "                       epochs=100, batch_size=32):\n",
        "    \"\"\"오프라인 RL DQN 학습\"\"\"\n",
        "    print(\"Starting offline DQN training...\")\n",
        "\n",
        "    state_dim = len(state_list[0])\n",
        "    action_dim = 3 #액션 개수 3개\n",
        "\n",
        "    # 네트워크 초기화\n",
        "    policy_net = DQN(state_dim, action_dim)\n",
        "    target_net = DQN(state_dim, action_dim)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
        "    gamma = 0.99\n",
        "    update_frequency = 10\n",
        "\n",
        "    # 전체 경험 리스트\n",
        "    dataset = list(zip(state_list, action_list, reward_list, next_state_list, done_list))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 무작위 배치 샘플링\n",
        "        batch = random.sample(dataset, batch_size)\n",
        "\n",
        "        states = torch.tensor([exp[0] for exp in batch], dtype=torch.float32)\n",
        "        actions = torch.tensor([exp[1] for exp in batch], dtype=torch.long)\n",
        "        rewards = torch.tensor([exp[2] for exp in batch], dtype=torch.float32)\n",
        "        next_states = torch.tensor([exp[3] for exp in batch], dtype=torch.float32)\n",
        "        dones = torch.tensor([exp[4] for exp in batch], dtype=torch.bool)\n",
        "\n",
        "        # Q-러닝 업데이트\n",
        "        current_q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = target_net(next_states).max(1)[0]\n",
        "            target_q_values = rewards + gamma * (1 - dones.float()) * next_q_values\n",
        "\n",
        "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 타겟 네트워크 주기적 업데이트\n",
        "        if epoch % update_frequency == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        # 학습 진행 출력\n",
        "        #if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(\"Offline DQN training completed!\")\n",
        "    return policy_net"
      ],
      "metadata": {
        "id": "Ni7jKZwDRKCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 비디오에서 프레임 읽기\n",
        "video_path = \"/content/drive/MyDrive/jolup/2_2.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "frames = []\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frames.append(frame)\n",
        "cap.release()\n",
        "print(f\"총 {len(frames)} 프레임 읽음\")\n",
        "\n",
        "# 2. LaneDetector, ObstacleDetector 생성\n",
        "lane_detector = LaneDetector()\n",
        "obstacle_detector = ObstacleDetector()\n",
        "\n",
        "# 3. OfflineDataCollector 생성\n",
        "collector = OfflineDataCollector(lane_detector, obstacle_detector)\n",
        "\n",
        "# 4. 데이터 수집\n",
        "state_list, action_list, reward_list, next_state_list, done_list = collector.collect_from_frames(frames)\n",
        "\n",
        "# 5. 결과 확인\n",
        "print(\"video_path\",video_path)\n",
        "print(f\"총 transition 수: {len(state_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKkz6vEOZSjp",
        "outputId": "1db1d8f0-13b5-408b-a802-7d38c054a037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 638 프레임 읽음\n",
            "샘플 state: [0.04453125 0.82109374 0.4328125  0.5        0.00096365]\n",
            "샘플 action: 0\n",
            "샘플 reward: 15.0\n",
            "샘플 next_state: [0.05703125 0.846875   0.4519531  0.5        0.00094626]\n",
            "샘플 done: False\n",
            "총 transition 수: 159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = train_offline_dqn(\n",
        "    state_list,\n",
        "    action_list,\n",
        "    reward_list,\n",
        "    next_state_list,\n",
        "    done_list,\n",
        "    epochs=3,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAodUIOLZmAK",
        "outputId": "a2b7feb3-fd07-415c-fbc0-0b20f8148789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting offline DQN training...\n",
            "Epoch 0, Loss: 149.4779\n",
            "Epoch 1, Loss: 167.9412\n",
            "Epoch 2, Loss: 161.8736\n",
            "Offline DQN training completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3199034132.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.tensor([exp[0] for exp in batch], dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예: 상태 입력으로 Q값 확인\n",
        "for i in range(159):\n",
        "  sample_state = torch.tensor(state_list[35], dtype=torch.float32)\n",
        "  q_values = policy_net(sample_state.unsqueeze(0))  # 배치 차원 추가\n",
        "  print(\"Sample Q-values:\", q_values.detach().numpy())\n",
        "  action = q_values.argmax().item()\n",
        "  print(\"Sample action:\", action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXeuI1kcZpv3",
        "outputId": "14d257fe-c26a-4820-9b10-b261e64bf6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Q-values: [[ 0.0983123   0.27721366 -0.04909869]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.06658914  0.25669318 -0.08162481]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05645216  0.2225334  -0.12108187]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04353227  0.2856777  -0.13794778]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.13933015  0.20064373 -0.05212199]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07662479  0.31854612 -0.10721801]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10960927  0.13965955 -0.03002194]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.00314743 0.30153975 0.00142593]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05124185  0.22152644 -0.0588759 ]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.04348784 0.2587208  0.00813333]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.00830344  0.31969324 -0.03618594]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.00394356  0.2602191  -0.07950318]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.11152954  0.20450477 -0.05634744]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09664173  0.2412095  -0.05279201]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04905324  0.347778   -0.02722747]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.01383469  0.27013168 -0.06746528]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.01752818  0.335184   -0.05120179]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.02119038  0.25029972 -0.05089664]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10852218  0.16569844 -0.05430806]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.00047899  0.29124147 -0.02111408]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0402791   0.3032654  -0.08648501]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0972802   0.26543847 -0.12004303]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.13052998  0.15029669 -0.05529501]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.01214559  0.22103257 -0.05383155]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09772732  0.2575691  -0.05463191]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09078153  0.25061357 -0.06088653]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04519702  0.25806808 -0.05351537]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07375559  0.19255787 -0.04526207]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.01696906  0.24155802 -0.0215744 ]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04737174  0.26078507 -0.09090715]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09041299  0.25682843 -0.01152104]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.00776267  0.3562071  -0.11526818]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.08796599  0.28924596 -0.06627046]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0066056   0.24614048 -0.08228526]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.01481319  0.2646235  -0.10824417]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10684046  0.24745345 -0.03967639]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.04405176 0.28402656 0.04139997]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04568547  0.3287936  -0.07315942]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.14150283  0.15577352 -0.09680224]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05708608  0.18318722 -0.05369528]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09903228  0.13372368 -0.03705612]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.06881943  0.20842561 -0.06170616]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.06144534  0.14960487 -0.04276219]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09968916  0.23045564 -0.07018365]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.13178656  0.21165678 -0.03551465]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.02041952  0.3119687  -0.12095176]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10121104  0.31071717 -0.03884353]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.00387191  0.2807958  -0.10772628]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.06483227  0.2976727  -0.08381469]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04669942  0.14164396 -0.05351625]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.11364464  0.29365724 -0.07801452]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10824386  0.1899645  -0.02547712]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.01811109  0.28256896  0.01989628]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.11351378  0.30159518 -0.07885671]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.02153741  0.29712528 -0.07365338]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.08179208  0.2525922  -0.00625399]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05123214  0.18145956 -0.08779287]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.06980817 0.1578826  0.01332302]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0122383   0.25409794 -0.00427432]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.04812956  0.30782288 -0.11269645]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03034522  0.28593317 -0.07359011]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.02886532  0.29280037 -0.10804749]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03646678  0.24909279 -0.03632998]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.06571552  0.3129593  -0.05331866]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03503258  0.28114173 -0.02523676]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.06755922  0.21319637 -0.09934271]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10153531  0.26242042 -0.06737018]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09475059  0.25149295 -0.08419698]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07269413  0.18326469 -0.04785798]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0294883   0.2563945  -0.09312027]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.02629534  0.2798838  -0.12423351]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.13705769  0.1346408  -0.07544627]]\n",
            "Sample action: 0\n",
            "Sample Q-values: [[-0.02606832  0.25925252 -0.02527991]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10363539  0.31567502 -0.07652155]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03950111  0.35801154 -0.10970219]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.0127039   0.28229603 -0.05268523]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.1230624   0.13665763 -0.09277713]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.11571291  0.18489794 -0.09755777]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.05034139 0.3496366  0.00731566]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.1719574   0.16208613 -0.1215687 ]]\n",
            "Sample action: 0\n",
            "Sample Q-values: [[ 0.1318559   0.2738992  -0.11172608]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.01141721  0.2540245  -0.03654354]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.02303094  0.32097095 -0.0462364 ]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03109138  0.23183975 -0.07146345]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03319479  0.27501857 -0.0149158 ]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04994298  0.33932233 -0.12874603]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04500693  0.32611027 -0.04576503]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10021709  0.16335543 -0.08066729]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0741136   0.2128946  -0.00960942]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.10880326 0.13860048 0.01711462]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.02210086  0.2857949  -0.00354328]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.12316243  0.14618354 -0.06001826]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.11329713  0.19591317 -0.03344619]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.08728029  0.28447387 -0.05642452]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07771062  0.27586424 -0.0003704 ]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05997165  0.26105973 -0.11314844]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.12501055 0.06136655 0.02347315]]\n",
            "Sample action: 0\n",
            "Sample Q-values: [[ 0.08552147  0.15676007 -0.00615641]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.06532988 0.20730424 0.03886503]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.01224328  0.24084061 -0.07590616]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04860153  0.15382396 -0.04845366]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.13439214  0.09003211 -0.06792898]]\n",
            "Sample action: 0\n",
            "Sample Q-values: [[ 0.05701003  0.29296154 -0.13980299]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.1463632   0.18716142 -0.08481952]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09245986  0.30057782 -0.07297227]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05554198  0.25740388 -0.00928389]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.11098187  0.14004189 -0.06471746]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04591937  0.2908194  -0.01828558]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.08195401  0.25672394 -0.12204763]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.00613738  0.26860124 -0.06560883]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10880887  0.19058353 -0.05826357]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.03863221 0.23970366 0.02443133]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.02704188  0.27616975 -0.04598311]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07060318  0.17802963 -0.08456331]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0481163   0.3367706  -0.04810357]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09152281  0.1890693  -0.01229629]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04378884  0.3051261  -0.10458289]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04396931  0.37180147 -0.06263255]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07200183  0.32318443 -0.09886254]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.01022661  0.32564774 -0.05329783]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07418372  0.27378818 -0.08776014]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.11390459  0.2405358  -0.0359066 ]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07018131  0.18471162 -0.03739206]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03700981  0.21887374 -0.03328016]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.01268641  0.302388   -0.08332349]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05074308  0.25237378 -0.07615895]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07298721  0.18086697 -0.14659438]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.10134163  0.2590613  -0.0498755 ]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05872358  0.3169929  -0.05665154]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05643582  0.30617896 -0.04949471]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05929045  0.12721485 -0.00625085]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05378242  0.16637334 -0.04701079]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.02578322 0.23955455 0.02467648]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09903474  0.3024857  -0.00346538]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.0275313   0.26733676 -0.0914623 ]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[0.09180842 0.14996621 0.00069312]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0667605   0.3142205  -0.05005191]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.1056684   0.31007943 -0.11179826]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05001858  0.16283113 -0.05175773]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.06496972  0.21123067 -0.05296985]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.07984032  0.23998842 -0.07132178]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.00278259  0.25720072 -0.09490436]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04980095  0.25243387 -0.09609907]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.0554283   0.18036681 -0.05721368]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.12319919  0.19739833 -0.12365197]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04394216  0.21018676 -0.08404124]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03566814  0.2653831  -0.08167691]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09991311  0.1663692  -0.01556186]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.15270583  0.1921896  -0.02018524]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.1445253   0.20503661 -0.03230991]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.06685523  0.24338982 -0.11339086]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.01033854  0.33230653 -0.08795663]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.03644359  0.2473413  -0.00831178]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.04857049  0.32081863 -0.13865969]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.09411507  0.21357206 -0.03369609]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.02079435  0.37178513 -0.05333642]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.05312049  0.31956843 -0.07833005]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[-0.00240299  0.30838943  0.01189371]]\n",
            "Sample action: 1\n",
            "Sample Q-values: [[ 0.00151268  0.20885101 -0.00751897]]\n",
            "Sample action: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reward_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X80_m7ufC01",
        "outputId": "6d47cf8b-d2e2-4cd0-e1af-00ffd7777eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 10.0, 15.0, 10.0, 10.0, 10.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 10.0, 10.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 0.0, 0.0, 0.0, 0.0, -20.0, 0.0, 0.0, -20.0, -20.0, 10.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 10.0, 10.0, 10.0, 10.0, 15.0, 15.0, 15.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 15.0, 15.0, 15.0, 10.0, 10.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U93y8qp8Ce87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}